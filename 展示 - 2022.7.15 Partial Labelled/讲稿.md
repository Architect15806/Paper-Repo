最近我重点看了这三篇文章

——————————

三篇文章共同想解决一个问题

就是我们如果要进行多器官分割的话，按传统方法需要有同时标注多器官的数据集

但是多器官标注在需求上比较少，在成本上比较大，所以这样的数据集并不多

那么我们就需要从一系列的单器官标注数据集上来训练一个能完成多器官分割的网络，这三篇文章就要解决这个问题

——————————

第一篇是一个co-training的网络

网络的运作过程可以简单的分为三个阶段

——————————

第一阶段

先在一系列单器官分割数据集上用同一个网络结构分别训练各个器官的分割网络

然后利用这些网络对其他所有数据集进行预测，将其补全成一个完整标注的多器官数据集

比如第k个器官

保留其原有的关于器官 k 的 ground truth

通过已训练的单器官网络计算出其他器官硬伪标签

然后将所有标签合并，则得到了一个带有硬伪标签的多器官数据集

——————————

第二阶段

训练一对分割网络，网络结构与之前训练的一致，输入同为全部的数据集，参数随机

这对网络相互利用对方输出的软标签和之前标注的硬伪标签进行训练



TWA操作加上网络互相监督，共同训练，这样避免每个网络使用自己以前的迭代预测作为监督，从而避免放大其以前迭代的分割错误




整体来看，一方面数据进入原始网络，将其预测直接与第一阶段生成的全标注的标签进行loss计算，得到图中的hard loss

hard loss由focal loss和dice loss组成，其中focal loss，体现在公式中的指数gamma，作用是：

通过调整参数，把正确分类的样本像素在整体中所占的权重降低，提高错误分类的像素的权重，增加错误分类的像素对整体loss产生的影响，从而解决样本分布失衡时（错误分类像素占极少数时）损失函数的分布发生倾斜的问题

——————————
**soft loss 的作用**

另一方面，在计算hard loss中使用的硬伪标签是有很大噪声的，所以为了进一步修正训练过程，防止分割的错误在迭代中被不断放大，需要让两个网络通过软伪标签的方式相互监督

如何让产生的软伪标签更为可靠？

使用TWA操作，这样的操作的意思是把网络新训练得到的参数与之前已有的参数进行加权平均，降低最新得到的网络参数的比重，从而使网络能产生鲁棒性更强的软标签

在本来就有ground truth的区域，软标签不如ground truth可靠，这些区域应该交由带ground truth的硬标签来监督

所以为了避开这些区域，添加一个区域掩码，只将软标签的loss计算作用于不存在ground truth标注的器官或背景的区域

将这对软标签进行loss的计算得到图中的soft loss

把之前计算得到的三种loss加权求和得到最后用于优化网络的loss，这样完成网络的训练

总体来看，在ground truth标注的范围内，使用ground truth进行监督，其他区域使用软硬伪标签进行监督

——————————

第三阶段就是网络的部署

把训练好的网络经过TWA后取出，就能直接用于多器官分割了，因为两个网络是对称的，所以拿哪一个都一样

——————————

所以总结下来，cotraining 网络的中心思想就是这四点（念）

——————————

接下来是第二篇dodnet

这个是dodnet的总图

——————————

首先来看对于这个问题，其他网络的做法（念）

——————————

本文所构建的动态按需网络dodnet就是在这个基础上优化而来

可以将整个网络简单分成这几个模块（念）

——————————

基础的U型网络和之前讲过的一系列网络基本一样

这里在图像压缩到最小尺寸时进行了一个gap操作，gap操作的作用是类似于全连接层，就是从张亮中提取一个特征向量出来，相比之下优势（念）

提取的目的是为了将这个图像的特征和任务信息融合

我们把将k个分割任务进行one-hot编码，形成k长度特征向量，将其与GAP输出拼接成更长的特征向量

——————————

这个特征向量用来生成动态头卷积核的参数，我们设计的动态头一共有三层，所以生成三个参数

然后将这三个参数装入动态头

动态头是连续三个1x1x1卷积，通道数8 8 2，因为原文是要针对器官和肿瘤分割的，所以输出通道是2个

这就是网络的训练过程

——————————

网络部署（念）

中心思想：提高共享（念）

**图像信息与任务信息拼接**

——————————

接下来是第三篇

在讲之前先来讲一下知识蒸馏模型，也就是教师-学生模型（念）

软标签和硬标签（念）

MNIST数据集 手写体数字

某个输入的“2”更加形似"3"，softmax的输出值中"3"对应的概率会比其他负标签类别高；

而另一个"2"更加形似"7"，则这个样本分配给"7"对应的概率会比其他负标签类别高。

Hard-target的值是相同的，但是 Soft-target却是不同的

由此我们可见Soft-target蕴含着比Hard-target更多的信息

在使用 Soft-target 训练时，Student模型可以很快学习到 Teacher模型的推理过程，从而拥有更好的泛化能力。

——————————

知识蒸馏有两种监督的方式

一种是从结果上进行logit wise 监督（念）

另一种是从过程上进行feature监督（念）

——————————

多教师单学生知识提炼框架，也就是mskd网络有以下这么几个要点（念）

——————————

然后是这个网络的两种监督方式

Logits-wise监督负责保证学生结果的一致性（念）

feature-wise监督负责保证学生训练过程的一致性（念）

——————————

中心思想（念）

——————————

总结一下这三个网络

同样目的

