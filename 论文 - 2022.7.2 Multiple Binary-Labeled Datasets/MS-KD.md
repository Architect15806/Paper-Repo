# MS-KD：用多个二进制标记的数据集进行多器官分割

## 〇、摘要

对三维医学图像中的多个器官进行注释是非常耗时和昂贵的，而多器官注释的数据集更是少之又少。

本文研究了**如何利用一组二元标记的数据集来学习多器官分割模型**，提出了一个新颖的多教师单学生知识提炼（Multi-teacher Single-student Knowledge Distillation, MS-KD）框架

**教师模型是预训练的单器官分割网络，学生模型是多器官分割网络**

考虑到每位教师关注不同的器官，我们提出了一种基于区域的监督方法，包括 logits 监督（logits-wise supervision）和特征监督（feature-wise supervision）。每位教师在两个区域监督学生，即教师被认为是专家的器官区域和所有教师都同意的后台区域。在三个公共的单器官数据集和一个多器官数据集上进行的大量实验证明了所提出的MS-KD框架的有效性。

### 知识蒸馏（教师-学生模型）

> https://blog.csdn.net/qq_38276972/article/details/117791237

<center>
<img src="知识蒸馏模型.png" width="60%" />
</center>
<center>图：知识蒸馏模型</center>

* 大模型往往是单个复杂网络或者是若干网络的集合，拥有良好的性能和泛化能力
* 小模型因为网络规模较小，表达能力有限

因此，可以**利用大模型学习到的知识去指导小模型训练**，使得小模型具有与大模型相当的性能，但是参数数量大幅降低，从而实现**模型压缩与加速**

#### 1. 基本框架

知识蒸馏采取 Teacher-Student 模式：将复杂且大的模型作为 Teacher，Student 模型结构较为简单，用 Teacher 来辅助 Student 模型的训练，Teacher 学习能力强，可以将它学到的知识迁移给学习能力相对弱的 Student 模型，以此来增强 Student 模型的泛化能力。复杂笨重但是效果好的 Teacher 模型不上线，单纯担当导师角色，真正部署上线进行预测任务的是灵活轻巧的 Student 小模型。

知识蒸馏是对**模型的能力进行迁移**，根据迁移的方法不同可以简单分为两大的方向：

* 基于目标蒸馏（也称为 Soft-target 蒸馏或 Logits 方法蒸馏）
* 基于特征蒸馏的算法

#### 2. 目标蒸馏 - Logits方法

分类问题的共同点是模型最后会有一个 softmax 层，其输出值对应了**相应类别的概率值**

由于已经有了一个泛化能力较强的 Teacher 模型，在利用 Teacher 模型来蒸馏训练 Student 模型时，可以直接让 Student 模型去学习 Teacher 模型的泛化能力

一个很直白且高效的迁移泛化能力的方法就是：_直接使用 softmax 层的输出（即类别的概率，并不归纳为 one-hot 类别）来作为Soft-target（也可以理解为软标签）_。

#### 3. Hard-target 和 Soft-target

概念类似于硬标签和软标签

传统的神经网络训练方法是定义一个损失函数，使预测值尽可能接近于真实值（Hard-target），这种训练过程是对 ground truth 求极大似然。在知识蒸馏中，是使用大模型的类别概率作为 Soft-target 的训练过程。

* Hard-target：原始数据集标注的 one-shot 标签，除了正标签为 1，其他负标签都是 0。
* Soft-target：Teacher 模型 softmax 层输出的类别概率，每个类别都分配了概率，正标签的概率最高。

知识蒸馏即用 Teacher 模型预测的 Soft-target 来辅助 Hard-target 训练 Student 模型

##### 1）方法的有效性

softmax 层的输出，除了正例（one-hot 编码为 1）之外，**负标签也带有大量的 Teacher 模型归纳推理信息**，比如某些负标签对应的概率远远大于其他负标签，则代表 Teacher模型在推理时认为该样本与该负标签有一定的相似性。而在传统的训练过程（使用 Hard-target）中，所有负标签都被统一对待（one-hot 编码为 0）。也就是说，知识蒸馏的训练方式使得每个样本给 Student 模型带来的**信息量大于传统的训练方式**

_【这里由于 softmax 计算方法得到的概率没有负概率，所以我理解作者说的“负标签”指的是概率非最大的其他所有标签，在一个像素的分类中，只有一个是正标签，其他都是负标签】_

如在 MNIST 手写体数字数据集中，若某个“2”更加形似"3"，softmax 的输出值中"3"对应的负标签概率会是所有负标签中最高的，这样的 Soft-target 明显蕴含着比 Hard-target 更多的信息

* 使用 Soft-target 训练时，Student 模型可以很快学习到 Teacher 模型的推理过程
* 使用 Hard-target 训练时，Student 模型只能学习到推理结果

##### 2）其他优势

* Soft-target 给 Student 模型带来的信息量要大于 Hard-target，并且 Soft-target 分布的熵相对高时，其 Soft-target 蕴含的知识就更丰富

* 使用 Soft-target 训练时，梯度的方差会更小，训练时可以使用**更大的学习率**，**所需要的样本更少**（这也解释了为什么通过蒸馏的方法训练出的 Student 模型相比使用完全相同的模型结构和训练数据只使用 Hard-target 的训练方法得到的模型，拥有**更好的泛化能力**）


#### 4. 具体方法

##### 1）Logits

在分类网络最后 Softmax 层之前，会得到评价各个类别的分值，某个类别的数值越大，则模型认为输入属于这个类别的可能性就越大。这些汇总了网络内部各种信息后，得出的属于各个类别的汇总分值，就是 Logits

因为 Logits 并非概率值，所以对其用 Softmax 函数进行变换，得出的概率值作为最终分类结果概率。Softmax 作用有：

* 把 Logits 数值在各类别之间进行概率归一，使得各个类别归属数值满足概率分布
* 放大 Logits 数值之间的差异，使得Logits得分**两极分化**，Logits得分高的得到的概率值更偏大一些，而较低的Logits数值，得到的概率值则更小。

##### 2）温度


softmax 层带来的两极分化会导致如果直接使用 softmax 层的输出值作为 soft-target，那么当 softmax 输出的概率分布熵相对较小时，负标签的值都很接近 0，对损失函数的贡献会非常小，小到可以忽略不计，soft-target也失去了意义

因此为原本的 softmax 增加温度变量 $T$:

$$q_i = \frac{exp(z_i/T)}{\sum_j exp(z_j/T)}$$

* $q_i$ - 属于第 $i$ 个类别的概率
* $z_j$ - 当前样本对于类别 $j$ 的 Logits
* $T$ - 温度变量
  * 当 $T=1$ 时，是标准的 Softmax 公式
  * $T$ 越高，softmax 的输出概率分布越趋于平滑，其分布的熵越大，**负标签携带的信息会被相对地放大**，模型训练将更加关注负标签。

##### 3）步骤

<center>
<img src="知识蒸馏步骤.png" width="80%" />
</center>
<center>图：知识蒸馏步骤</center>

`->` 训练 Teacher 模型

`->` 利用高温 $T_{high}$ 产生 soft-target 和低温 $T=1$ 产生 hard-target

`->` 使用 $\{T_{high}, soft-target\}$ 和 $\{T=1, hard-target\}$ 同时训练 Student 模型

`->` 使用 $T=1$ 让 Student 模型上线



## 一、简介

三维医学图像分割的注释成本相当高，大多数基准数据集只提供单一器官的分割 mask。

为了解决这个问题，最近的一些研究提出利用多个单一器官的数据集来训练一个多器官的分割模型。基于条件的方法将每个分割任务编码为任务意识，以指导网络对任务相关的器官进行分割。提出了几种调节网络的方法，如将器官类别信息纳入中间激活信号，或将任务意识先验纳入动态按需网络（DoDNet）生成动态卷积滤波器。

然而，上述基于条件的方法需要几轮推理才能得到最终的分割结果，这在**计算上是低效的**。

* 基于伪标签的方法使用预先训练好的单器官模型， [12,4]为每个数据集中未注释的器官生成伪标签，然后构建一个伪多器官数据集，用来训练多器官分割模型。
* 由于伪标签的质量至关重要，文献[12]中引入了使伪标签的器官大小分布规律化的惩罚，文献[4]中提出了一对权重平均的模型，其中每个模型都用另一个模型的输出进行训练，以获得更可靠的伪标签。
* 现有的基于伪标签的方法大多只利用多个预训练的单器官模型的二进制输出。通过一致性损失从软性伪标签中学习已被证明比从硬性标签中学习更有效[13]。

本文探索一个知识提炼框架，以学习**结合一组单器官数据集来分割多个器官**。

采用师生模型，广泛用于图像分类和分割任务的知识提炼，利用教师模型中的**软伪标签**或**中间特征**来训练学生模型。

本文结构中有多个教师模型，每个模型都是一个**单器官的分割模型**。

本文的设置与典型的师生模型不同：教师的数量不同，且每个教师网络都是针对不同的单器官分割任务，他们共同教导学生分割多个器官（如下图）

<center>
<img src="网络结构对比.png" width="90%" />
</center>
<center>图：网络结构对比</center>

多教师单学生知识提炼（Multi-teacher Single-student Knowledge Distillation, MS-KD）框架，将教师模型提炼成单一的学生模型用于多器官分割。我们提出了一个基于区域的监督，使学生能够从各自的老师那里学习每项任务

学生从各自的老师那里学习分割器官，从所有的老师那里学习分割背景，因此每个老师在两个区域监督学生，即**老师预测的器官区域**和**所有老师都同意的背景区域**，每个区域都采用 Logits 监督和特征监督的方式：

* Logits 监督：教师网络和学生网络的 Logits 是不同的，即教师 2 个，学生 K+1 个，K 是需要分割的器官数量。对教师的 Logits 输出进行简单的维度转移，用于约束学生

* 特征监督：学生的中间特征的分布是根据教师的中间特征来制约的

本文贡献：

* 提出 MS-KD 框架，从多个单器官分割网络中训练一个多器官分割网络
* 提出了基于区域的监督方法，包括 Logits 监督和特征监督，以使学生模型能够从多个任务不同的教师那里学习

## 二、方法

单器官标注数据集 $\{D_1, D_2, ..., D_K\}$

$D_k$ 数据集中包含图像 $X^k$ 和对应的器官 $k$ 的硬标注 $Y^k$（one-hot 编码）

### （一）多教师单学生知识蒸馏法（MS-KD）

<center>
<img src="MSKD模型.png" width="90%" />
</center>
<center>图：MSKD 模型</center>

$K$ 个独立的教师网络：$\{f_{T_1}, f_{T_2}, ..., f_{T_K}\}$，每个网络有独立的损失函数：

$$\min_{\theta_k} \sum_{i=1}^{N_k} L(f_k(x_i^k;\theta_k), y_i^k)$$

* $\theta_k$ - 教师网络 $k$ 的参数
* $L$ - dice loss 与交叉熵 loss 的结合

学生网络输出通道为 $K+1$，教师网络输出通道为 $2$，除此之外结构相同

当训练学生网络时，教师网络被冻结

### （二）基于区域的监督

$M^k(x_i)$ - 教师网络 $k$ 对于像素 $x_i$ 的 one-hot 编码的预测结果（背景 - 0，器官 - 1）

教师 $k$ 的背景预测实际上是真正的背景区域和 $K-1$ 个不相关的器官区域的集合，所以只能相信教师对相应器官区域的预测

考虑到器官对于空间的占用一定是互斥的（没有重叠的器官存在），可以提出了一个基于区域的监督策略来解决以上难题

首先定义两组区域：

* 器官 $k$ 区域：像素 $M^k(x_i) = 1$，此区域的监督让学生学到器官 $k$ 的分割
  * 代表教师 $k$ 是专家的特定区域
* 真正的背景区域：像素 $\forall k M^k(x_i) = 0$（即在所有图像上都为背景的像素），真正的背景区域是所有教师的背景区域相交而得到的
  * 代表所有教师的集体智慧所在

将这两组区域标识成 mask 的形式：

* 在教师网络 $k$ 上：

$$M_i^k = \begin{equation}
\begin{cases}
1,\quad 像素 i 位置是器官标注\\
0,\quad 像素 i 位置是背景标注\\
\end{cases}
\nonumber
\end{equation} $$

* 在全部的教师网络中：

$$M^B = \begin{equation}
\begin{cases}
1,\quad 像素 i 位置是背景标注\\
0,\quad 像素 i 位置是某个器官标注\\
\end{cases}
\nonumber
\end{equation} $$

两个区域的交错使我们能够将教师对学生的影响限制在教师更确定是正确的区域

学生模型学习分割多个器官，汇总来自不重叠的专业教师的监督

#### 1. 基于区域的 Logits 监督

Logits 维度转移策略：

对于二分类问题的教师网络 $k$：

$$q_c^k = \frac{f_k(x_i^k;\theta_k)_c}{\sum_c exp(f_k(x_i^k;\theta_k)_c)} \ \ \ \ \ c\in\{0, 1\}$$

在这个教师网络中，$q_c^k$ 只有 $q_0^k$ 和 $q_1^k$，分别代表一个像素为背景或器官的概率

而对于学生网络，$q_c^k$ 记作 $\hat{q}_c^k$ 需要包含 $c\in \{0, 1, ..., K\}$

那么将 $q_c^k$ 扩展为 $\hat{q}_c^k$ 的方法为：

$$\hat{q}_c^k = \begin{equation}
\begin{cases}
q_0^k,\quad c=0\\
q_1^k,\quad c=k\\
0,\quad other\\
\end{cases}
\nonumber
\end{equation} $$

也就是在本教师管辖区域内保留，管辖区域外补 0

$\hat{q}_c^k$ 的含义：教师网络 $k$ 输送给学生网络的软标签，$c$ 指示当前标签是分割哪个器官/背景的，当且仅当 $c$ 指示教师专业的器官时有效

##### （1）器官 loss

于是在 $M_i^k$ 的限制下，计算学生网络预测 $q_i^S$ 和教师网络软标签 $\hat{q}_i^k$ 之间的器官 loss：

$$L_{logit}^k = \frac{1}{W\times H}\sum_{i \in R} KL(\hat{q}_i^k, q_i^S) \cdot M_i^k$$

* $R$ - 指定一张图像所有像素的下标范围
* $KL(\cdot)$ - KL 距离（Kullback-Leibler Divergence）

##### （2）背景 loss

将所有教师网络的输出软标签进行加权平均后，真正的背景 $\hat{q}_i^B$ 部分由所有教师共同产生：

$$\hat{q}_i^B = (\hat{q}^1 + \hat{q}^2 + ...+\hat{q}^K)/K$$

进而在 $M_i^B$ 的限制下计算学生网络预测 $q_i^S$ 和教师网络软标签 $\hat{q}_i^B$ 之间的背景 loss：

$$L_{logit}^B = \frac{1}{W\times H}\sum_{i \in R} KL(\hat{q}_i^B, q_i^S) \cdot M_i^B$$

#### 2. 基于区域的特征监督

此处要实现教师网络对学生网络在中间层的监督

$M^{k,l}$ mask 掩码表示教师网络 $k$ 中对图像进行下采样 $l$ 次后的器官位置二进制掩码

$$M^{k,l} = \begin{equation}
\begin{cases}
1,\quad 器官 k 在下采样 l 次后的位置\\
0,\quad 背景和其他器官在下采样 l 次后的位置\\
\end{cases}
\nonumber
\end{equation} $$

仅对于器官 $k$ 来说，教师网络和学生网络中的特征分布应该是相似的，但教师网络不能负责其他不擅长的区域，所以我们沿通道维度对每个像素的特征进行排序，只在器官 $k$ 的区域进行监督，特征 loss 计算为：

$$L_{feature}^{k,l} = \frac{1}{(W/2^l)\times (H/2^l)}\sum_{i \in R^l} KL(sort(F_i^{k,l}), sort(F_i^{S,l})) \cdot M_i^k$$

* $F_i^{k,l}$ $F_i^{S,l}$ - 教师网络 $k$ 和学生网络在第 $l$ 次下采样输出后经过 softmax 层的结果

#### 3. 总分割 loss

$$L = \sum_{k=1}^K L_{logit}^k + \lambda_1 L_{logit}^B + \lambda_2\sum_{k=1}^K L_{feature}^{k,l}$$

## 三、实验

### （一）参数设置

* 基础网络 backbone：2D U-Net
* 输入尺寸 input patch size:512x512
* 批次大小 batch size：4
* 迭代次数：250x4=1000 次
* 标注区域比例：33%
* 优化器：Adam
* 初始学习率：3e-4（当 loss 减少 1e-3 时，学习率下调 20%）
* 超参：$\lambda_1 = 1$、$\lambda_2 = 10$、$l = 1$