3D unet 是基于 2D unet 专门针对医学影像分割提出的，解决三个问题：



第一个问题：如何让已有的2d unet用于3d

直接将2d unet应用于3D 图像的问题：
1. 需要进行切片，匹配对应的mask，对每个切片进行训练，无法利用全部数据，无法完成端到端分割
2. 进行切割后无法考虑各层之间的上下文关系

为此需要构建一个直接处理3D图像的网络
对2D unet做出修改

其一，使用3d卷积和3d反卷积替代2d的相应操作

其二，使用步幅为2，卷积核尺寸为2的3d卷积代替2d 的 maxpooling
这样一方面增加可以训练的参数，加深网络，效果更好
另一方面可以减少计算量，将池操作替换为卷积操作可以使网络在训练期间内存占用更小

保留skip，使用unet的通道叠加后卷积的特征融合策略
保留overlap tile的图片边缘上下文处理，即输入尺寸大于输出尺寸

如此改进，3D-Unet可以采取图片整张作为输入到模型中进行分割，完成端到端的预测

第二个问题：如何避免数据表达的瓶颈（Bottleneck）

这个问题的提出是在GoogLeNet：Inception V3，本文的作者也参考了这个网络
Inception V3给出了避免这个问题的四项原则，其中一条是：

避免表达的瓶颈，表达的尺寸（即feature map的大小）不应该出现急剧的衰减
如果对流经层（尤其是卷积层）的信息过度的压缩，将会丢失大量的信息，对模型的训练也造成了困难。

在2d unet中就存在这一问题

按照我的理解，在2d unet中，
每次是先进行下采样，过程中通道数不变，尺寸减半
在进行通道的扩张，通道数加倍
在这个过程中可以理解为数据量先减半再加倍
这样会导致数据在特征汇集中一次性丢失过多



――――――――――――――――――――――

为此，3D unet将通道增加操作提到了尺寸缩减前

先在卷积中增加通道数，再进行下采样，将尺寸减半

――――――――――――――――――――――

第三个问题：如何合理利用稀疏标注的3D图像

首先3D unet是一个端到端的分割网络，就不能只使用标注的切片

方法是
计算loss时使用加权交叉熵损失（weighted cross-entropy loss function）
使用带有加权的交叉熵损失的像素 softmax 生成网络的预测分类，
标签为 unlabled 的体素对损失计算没有贡献，即权重为 0
这样让未标注的切片在计算loss时不会对整体产生影响，只比对标注部分与ground truth

其次，稀疏标注会带来数据量的不足，需要进行数据增强
如何合理利用稀疏标注的3D图像
主要由rotation、scaling和将图像设置为gray，
数据和ground truth上运用平滑的密集变形场(smooth dense deformation field)取样
然后应用B样条插值(B-Spline Interpolation)
我理解这个过程是基于原本3D图像的形状去找一个类似地形状来近似（approximation）

――――――――――――――――――――――

3D uent可以有两种使用方式：
从稀疏标注生成分割
从未标注生成分割


3D unet 和 vnet 我是看了论文但是还没去跑代码
最近几天一直在看和改inter slice的代码

――――――――――――――――――――――

前面所提到的两个网络，主要思想还是将2d的模型迁移到3d上来
没有就3d医学影像的特点做出上下文感知的调整，对 3D 上下文感知的能力有限
Conresnet 就这一点做出了改进

论文首先提出了一个层间残差的概念
就是确定3d图像的一个轴为深度方向
沿这个方向将分割的mask进行逐层相减得到层间残差

层间残差表征的是分割图所标注的物体在深度上的变化

――――――――――――――――――――――

这个图中

左边的是分割图对应的残差的表现
右侧是对一张图的各层的层间残差做平均值的结果
可以看到，残差描绘出了一个物体的边缘
物体在各层之间的变化越大，残差就越明显
残差可以很好的表征物体的位置特征

论文的一个思想就是
如果可以用某种机制，让分割网络将注意力集中在这些物体边缘部分，那么可以更好的提高准确率

――――――――――――――――――――――

这是conresnet的总流程图

可以看出它在3d unet的模型上进一步改进而来的

这里是一个unet的encoder-decoder结构，
但是从实验代码上看，这里是使用了FCN 的体素直接对应求和的方式进行的特征融合
这样也可以节约算力

click――――――――

在encoder结构内，conresnet借鉴vnet的做法，大量引入了残差的捷径
每个红色的模块都是这样一个带捷径的两个3x3x3的卷积块

这个encoder论文里叫shared encoder
即它的输入为原图和原图直接计算出的残差图，这两批数据分别编码训练
残差的计算方式为深度上的每一层直接减去它的下一层
这样直接减会导致结果缺一层，所以在最底层进行padding补一个全0层
所以生成的残差图与原图的尺寸完全一致

click――――――――

shared encoder输出编码后的原图和残差图
分别送入分割解码器和上下文残差解码器

上面的分割解码器的原理与unet的基本一致
但是在运作过程中是与下面的残差解码器一起作用的

首先是以元素相加的方式利用encoder收缩过程中的捷径

图中的加权层weight layer，都是包含了卷积、group normalization和relu激活函数

相加后数据通过weight layer生成分割特征图 F 

unet和vnet的做法是直接输出 F 到后一层解码器

但是conresnet在这里对 F 加入了注意力的加权机制

对 F 按照层间相减，底层补0的方式求出残差特征图 G

G 在经过加权层后与上一层传来的残差图进行特征融合

再经过一个加权层后作为新的残差解码器的输出

残差解码器的输出用来作为分割特征图 F 的注意力加权

但是为了防止残差信息影响过大，对其做 sigmoid ，限制在0，1区间内

再+1作为 F 的注意力加权，作为分割解码器的输出

这个过程可以有效的让物体边缘位置的体素的权重更高，有利于更准确的分割

――――――――――――――――――――――

我用的数据集是这个2018年脑瘤的数据集，共285张有标注的图像

在其中，我手动选择了20张作为验证集，其余作为训练集

在图像输入的时候由于图像非常大，常采用随机裁剪的方式缩减原图和标注为固定的尺寸
便于训练，也变相进行了数据增强

这个数据集中的每一组图片都是由一张原图、三张病灶的图，对应三种病ET WT TC
和一张分割结果图组成
在使用时，将原图和三张病灶的图叠加为四通道的3d图像
将单通道的分割图按照体素对应的值，分为由0和1组成的三通道图
每个通道对应一种病的mask

我在训练的时候，按照作者的代码裁剪为 80x160x160 的，
但是这样需要一个多G的显存，显存不够用
所以我就把随机裁剪尺寸缩一半到 40x80x80 来训练

网络用来训练的数据指标是综合了dice loss和交叉熵BCE得出的

这里使用的交叉熵采用残差解码器每一层的输出的加权求和

――――――――――――――――――――――

这是我跑代码出来的结果

分别统计了三种病灶的dice score

在loss上分别统计了分割的loss和残差的loss

可以看到loss方面下降比较正常

但是dice score就有点低了

我怀疑是我的数据集划分有问题，或者是随机裁剪的尺寸设置太小了

这些我打算接下来一段时间去验证一下

――――――――――――――――――――――

今后的计划









之前很多的方法都是只能处理2D图像，而在临床的实践中很多都是包含3D体积。因此提出了一种基于体积、基于FCN的三维图像分割方法。

这种分割是通过仅考虑局部上下文来获得的，因此容易失败，尤其是在具有挑战性的模式中，例如超声，其中会出现大量错误分类的体素

我们引入了一个新的目标函数，我们在训练期间根据 Dice 系数对其进行优化。通过这种方式，我们可以处理前景和背景体素数量之间存在严重不平衡的情况。


在每个阶段执行的卷积使用大小为 5×5×5 体素的体积核。
随着数据沿压缩路径通过不同阶段，其分辨率会降低。这是通过使用步幅为 2 的 2 × 2 × 2 体素宽内核的卷积来执行的（图 3）。
由于第二个操作仅通过考虑不重叠的 2×2×2 体积块来提取特征，因此生成的特征图的大小减半。这种策略与池化层的目的相似，
受 [16] 和其他不鼓励在 CNN 中使用最大池化操作的工作的推动，在我们的方法中已被卷积层取代。
此外，由于 V-Net 压缩路径的每个阶段的特征通道数量加倍，并且由于将模型表述为残差网络，因此我们采用这些卷积操作将特征映射的数量加倍，因为我们降低他们的分辨率。 
PReLu 非线性应用于整个网络。